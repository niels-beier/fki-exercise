{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise1 Text Modelling & Natural Language Understanding\n",
    "\n",
    "This exercise sheet will serve as supplementary to lectures and help you to familiarize yourself with Python, Tensroflow, and other commonly used packages.\n",
    "\n",
    "If you are not familiar with Python, you may want to learn more about Python\n",
    "and its basic syntax. Since there are a lot of free and well-written tutorials\n",
    " online, we refer you to one of the following online tutorials:\n",
    "\n",
    "* http://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\n",
    "* https://www.learnpython.org/\n",
    "* https://www.w3schools.com/python/\n",
    "* https://automatetheboringstuff.com/\n"
   ],
   "metadata": {
    "id": "sDJf4y-6u4qZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bag of Words"
   ],
   "metadata": {
    "id": "UJCcM2ZzitcD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A bag of words is a representation of text that describes the occurrence of words within the corpus. Here we are going to have a hands-on exercise to better understand this approach.\n",
    "\n",
    "Given the following sentences:\n",
    "- OpenAI developed ChatGPT.\n",
    "- ChatGPT is a large language model.\n",
    "- Language technology is interestng.\n",
    "\n",
    "**Question**:\n",
    "\n",
    "\n",
    "1.   What is the vocabulary?\n",
    "2.   Encode these sentences with the vocabulary.\n",
    "\n",
    "**Answers**:\n",
    "1.   openai, developed, chatgpt, is, a, large, language, model, technology, interesting\n",
    "2.   - {\"openai\": 1, \"developed\": 1}\n",
    "     - {\"chatgpt\": 1, \"is\": 1, \"a\": 1, \"large\": 1, \"language\": 1, \"model\": 1}\n",
    "     - {\"language\": 1, \"technology\": 1, \"is\": 1, \"interesting\": 1}"
   ],
   "metadata": {
    "id": "w9EZIi5Ii8qX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### N-gram Language Model\n",
    "\n",
    "Next, we are going to have a coding exercise about the n-gram language model. We have a corpus that is one chapter from a book and has almost 1000 lines. In this exercise, rather than building every function from scratch, we are going to use the toolkit NLTK, which is a suite of open-source Python modules, data sets, and tutorials supporting research and development in Natural Language Processing.\n",
    "\n",
    "For more information about NLTK, please refer to the official documentation: https://www.nltk.org/\n"
   ],
   "metadata": {
    "id": "fmCYuwngiydr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Firstly, let's download the data for this exercises\n",
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
   ],
   "metadata": {
    "id": "yzweuCiE9478",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:31:17.756066Z",
     "start_time": "2025-05-05T14:31:17.375759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-05 16:31:17--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8000::154, 2606:50c0:8002::154, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 278779 (272K) [text/plain]\r\n",
      "Saving to: ‘botchan.txt’\r\n",
      "\r\n",
      "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.04s   \r\n",
      "\r\n",
      "2025-05-05 16:31:17 (7.53 MB/s) - ‘botchan.txt’ saved [278779/278779]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# If you are running with colab, this toolkit should be installed.\n",
    "import nltk\n",
    "print(nltk.__version__)\n",
    "nltk.download('punkt_tab')"
   ],
   "metadata": {
    "id": "t8PT3d4E-OH2",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:36:02.961013Z",
     "start_time": "2025-05-05T14:36:02.408035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/niels/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Load punctuations that to be removed in later preprocessing.\n",
    "import nltk, re, pprint, string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "puncs = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
    "print(puncs)\n",
    "\n",
    "puncs = puncs.replace('.', '')\n",
    "print(puncs)\n"
   ],
   "metadata": {
    "id": "DCfC-cLa40wP",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:37:27.218311Z",
     "start_time": "2025-05-05T14:37:27.215247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~“”-’‘—\n",
      "!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~“”-’‘—\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the corpus\n",
    "file = open('botchan.txt', encoding = 'utf8').read()\n",
    "#preprocess data\n",
    "file_nl_removed = \"\"\n",
    "for line in file:\n",
    "  line_nl_removed = line.replace(\"\\n\", \" \")           #removes newlines\n",
    "  file_nl_removed += line_nl_removed\n",
    "\n",
    "file_p = \"\".join([char for char in file_nl_removed if char not in puncs])   #removes all special characters. Save the data as a string."
   ],
   "metadata": {
    "id": "y6lL2sNjFlUF",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:43:04.565376Z",
     "start_time": "2025-05-05T14:43:02.996353Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "file_p[:1000] # Display the first 1000 elements"
   ],
   "metadata": {
    "id": "WmWKBl7n6Sql",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:43:07.158874Z",
     "start_time": "2025-05-05T14:43:07.153597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffProject Gutenbergs Botchan Master Darling by Kinnosuke Natsume This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.  You may copy it give it away or reuse it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org Title Botchan Master Darling Author Kinnosuke Natsume Translator Yasotaro Morri Posting Date October 14 2012 EBook 8868 Release Date September 2005 First Posted August 17 2003 Language English  START OF THIS PROJECT GUTENBERG EBOOK BOTCHAN MASTER DARLING  Produced by David Starner and the Online Distributed Proofreading Team BOTCHAN MASTER DARLING By The Late Mr. Kinnosuke Natsume TRANSLATED By Yasotaro Morri Revised by J. R. KENNEDY 1919 A NOTE BY THE TRANSLATOR No translation can expect to equal much less to excel the original. The excellence of a translation can only be judged by noting how far it has succeeded in reproducing the original tone colors style the delicacy of senti'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "# Check some statistics about the data. Here you need to complete the codes with reference: https://www.nltk.org/api/nltk.tokenize.html\n",
    "sents = sent_tokenize(file_p)\n",
    "print(\"The number of sentences is\", len(sents)) #prints the number of sentences\n",
    "\n",
    "words = word_tokenize(file_p)\n",
    "print(\"The number of tokens is\", len(words)) #prints the number of tokens\n",
    "\n",
    "average_tokens = round(len(words)/len(sents))\n",
    "print(\"The average number of tokens per sentence is\", average_tokens) #prints the average number of tokens per sentence\n",
    "\n",
    "unique_tokens = set(words)\n",
    "print(\"The number of unique tokens are\", len(unique_tokens)) #prints the number of unique tokens"
   ],
   "metadata": {
    "id": "VV3pSjmY6Lgh",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:46:56.907206Z",
     "start_time": "2025-05-05T14:46:56.512380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 2787\n",
      "The number of tokens is 53568\n",
      "The average number of tokens per sentence is 19\n",
      "The number of unique tokens are 6293\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# Now let's collect ngrams\n",
    "from nltk.util import ngrams #https://www.nltk.org/_modules/nltk/util.html#ngrams\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "fourgram=[]\n",
    "tokenized_text = []\n",
    "\n",
    "for sentence in sents:\n",
    "    sentence = sentence.lower() # Lower the sentence for simplification\n",
    "    sequence = word_tokenize(sentence) # Word-level\n",
    "    for word in sequence:\n",
    "        if (word =='.'):\n",
    "            sequence.remove(word)\n",
    "        else:\n",
    "            unigram.append(word)\n",
    "    tokenized_text.append(sequence)\n",
    "    bigram.extend(list(ngrams(sequence, 2)))\n",
    "    trigram.extend(list(ngrams(sequence, 3)))\n",
    "    fourgram.extend(list(ngrams(sequence, 4)))"
   ],
   "metadata": {
    "id": "sfEL_tJTCfWn",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:48:56.696625Z",
     "start_time": "2025-05-05T14:48:56.362338Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'Lengths of unigram, bigram, trigram and fourgram are {len(unigram)}, {len(bigram)}, {len(trigram)}, {len(fourgram)}')"
   ],
   "metadata": {
    "id": "tZVHFOC2Ci2k",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:48:59.420134Z",
     "start_time": "2025-05-05T14:48:59.415914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unigram, bigram, trigram and fourgram are 50796, 48009, 45246, 42515\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# Inspect\n",
    "print(tokenized_text[:2])\n",
    "\n",
    "print(unigram[:5])\n",
    "print(bigram[:5])\n",
    "print(trigram[:5])\n",
    "print(fourgram[:5])"
   ],
   "metadata": {
    "id": "u6lAxDhKZcPa",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:49:08.345953Z",
     "start_time": "2025-05-05T14:49:08.339092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\\ufeffproject', 'gutenbergs', 'botchan', 'master', 'darling', 'by', 'kinnosuke', 'natsume', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever'], ['you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', 'title', 'botchan', 'master', 'darling', 'author', 'kinnosuke', 'natsume', 'translator', 'yasotaro', 'morri', 'posting', 'date', 'october', '14', '2012', 'ebook', '8868', 'release', 'date', 'september', '2005', 'first', 'posted', 'august', '17', '2003', 'language', 'english', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'botchan', 'master', 'darling', 'produced', 'by', 'david', 'starner', 'and', 'the', 'online', 'distributed', 'proofreading', 'team', 'botchan', 'master', 'darling', 'by', 'the', 'late', 'mr.', 'kinnosuke', 'natsume', 'translated', 'by', 'yasotaro', 'morri', 'revised', 'by', 'j.', 'r.', 'kennedy', '1919', 'a', 'note', 'by', 'the', 'translator', 'no', 'translation', 'can', 'expect', 'to', 'equal', 'much', 'less', 'to', 'excel', 'the', 'original']]\n",
      "['\\ufeffproject', 'gutenbergs', 'botchan', 'master', 'darling']\n",
      "[('\\ufeffproject', 'gutenbergs'), ('gutenbergs', 'botchan'), ('botchan', 'master'), ('master', 'darling'), ('darling', 'by')]\n",
      "[('\\ufeffproject', 'gutenbergs', 'botchan'), ('gutenbergs', 'botchan', 'master'), ('botchan', 'master', 'darling'), ('master', 'darling', 'by'), ('darling', 'by', 'kinnosuke')]\n",
      "[('\\ufeffproject', 'gutenbergs', 'botchan', 'master'), ('gutenbergs', 'botchan', 'master', 'darling'), ('botchan', 'master', 'darling', 'by'), ('master', 'darling', 'by', 'kinnosuke'), ('darling', 'by', 'kinnosuke', 'natsume')]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "freq_uni = nltk.FreqDist(unigram)\n",
    "freq_bi = nltk.FreqDist(bigram)\n",
    "freq_tri = nltk.FreqDist(trigram)\n",
    "freq_four = nltk.FreqDist(fourgram)\n",
    "\n",
    "print (\"Most common unigram: \", freq_uni.most_common(5))\n",
    "print (\"Most common bigrams: \", freq_bi.most_common(5))\n",
    "print (\"Most common trigrams: \", freq_tri.most_common(5))\n",
    "print (\"Most common fourgrams: \", freq_four.most_common(5))"
   ],
   "metadata": {
    "id": "aSbUY7cz-BhC",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:49:19.908923Z",
     "start_time": "2025-05-05T14:49:19.699919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common unigram:  [('the', 2755), ('i', 1714), ('to', 1472), ('and', 1310), ('of', 1258)]\n",
      "Most common bigrams:  [(('of', 'the'), 316), (('in', 'the'), 223), (('to', 'the'), 189), (('red', 'shirt'), 174), (('i', 'was'), 137)]\n",
      "Most common trigrams:  [(('i', 'did', 'not'), 39), (('it', 'would', 'be'), 32), (('i', 'could', 'not'), 28), (('of', 'red', 'shirt'), 24), (('to', 'the', 'school'), 21)]\n",
      "Most common fourgrams:  [(('one', 'sen', 'and', 'a'), 13), (('sen', 'and', 'a', 'half'), 13), (('the', 'project', 'gutenberg', 'literary'), 13), (('project', 'gutenberg', 'literary', 'archive'), 13), (('gutenberg', 'literary', 'archive', 'foundation'), 13)]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "# Predict the next word\n",
    "str1 = 'i like the'\n",
    "token1 = word_tokenize(str1)\n",
    "\n",
    "num_gram = 2  # what if 4, 5\n",
    "\n",
    "results = list(ngrams(token1, num_gram))\n",
    "search=results[-1]\n",
    "print('Searching ', search)\n",
    "preds=[]\n",
    "\n",
    "# Naive search with a for loop\n",
    "\n",
    "for each in trigram:\n",
    "  if each[:-1]==search:\n",
    "    preds.append(each[-1])\n",
    "print('Found ', preds)\n",
    "\n",
    "# What to predict?\n",
    "from collections import Counter\n",
    "word_counts = Counter(preds)\n",
    "print('Most Common words with counts is: ', word_counts.most_common(1))"
   ],
   "metadata": {
    "id": "GuyWRWLqcvwD",
    "ExecuteTime": {
     "end_time": "2025-05-05T14:52:52.494023Z",
     "start_time": "2025-05-05T14:52:52.469941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching  ('like', 'the')\n",
      "Found  ['cheap', 'undue', 'dwarf', 'name', 'wrestling', 'inmates', 'tradespeople', 'wrestling']\n",
      "Most Common words with counts is:  [('wrestling', 2)]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "# Other popular ngram LM with scoring functions\n",
    "\n",
    "# https://github.com/kmario23/KenLM-training"
   ],
   "metadata": {
    "id": "73PSq7f0g_Ft"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Byte Pair Encoding"
   ],
   "metadata": {
    "id": "K8HPYpBti4Fh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Byte Pair Encoding (BPE) was initially developed as an algorithm to compress data, and now is commonly used in natural language processing tasks such as machine translation and text generation. BPE starts by computing the unique set of characters used in the corpus, then building the vocabulary by taking all the symbols used to write those words.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "r-LxM6kpGqA8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Calcuation\n",
    "\n",
    "Here we are going to implement BPE with a simple corpus step by step.\n",
    "\n",
    "Given the corpus like the following:\n",
    "\n",
    "- happy * 3\n",
    "- lucky * 2\n",
    "- cool * 2\n",
    "- good * 2\n",
    "\n",
    "You can inteprete each word as a single sentence. For example, the split of the words happy as {h_a_p_p_y_/w}\n",
    "\n",
    "**Questions**:\n",
    "\n",
    "\n",
    "1.   What is the initial vocabulary? Hint: don't forget the token **/w** at the end of each word.\n",
    "2.   What is the merge operation in the first iteration?\n",
    "3.   Let's say we would like to have a vocabulary with size 15. Calculate the vocabulary and the merging rules.\n"
   ],
   "metadata": {
    "id": "lEtih0VP9tws"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Coding\n",
    "Next we are going to build a BPE model with [SentencePiece](https://github.com/google/sentencepiece/tree/master)."
   ],
   "metadata": {
    "id": "IGab9Cg19nzm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Coding exercise\n",
    "!pip install sentencepiece\n"
   ],
   "metadata": {
    "id": "eUMvy7QKjYYg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!head -n 3 botchan.txt # Display the first three rows"
   ],
   "metadata": {
    "id": "bkmTtaUqpoxi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import sentencepiece as spm\n",
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix='m', vocab_size=2000)\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(sp.encode_as_pieces('This is a test')))\n",
    "print(sp.decode_ids(sp.encode_as_ids('This is a test')))"
   ],
   "metadata": {
    "id": "otyovEjXqGLA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# returns vocab size\n",
    "print(sp.vocab_size())\n",
    "\n",
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for id in range(3):\n",
    "  print(sp.id_to_piece(id))"
   ],
   "metadata": {
    "id": "ZXwomFHwqUjN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# The impact of vocab size.\n",
    "for vocab_size in [100, 500, 5000]:\n",
    "  spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix='m', vocab_size=vocab_size)\n",
    "  sp = spm.SentencePieceProcessor()\n",
    "  sp.load('m.model')\n",
    "  print(sp.encode_as_pieces('This is exercise 1'))\n",
    "\n",
    " # What if vocab_size is 50?"
   ],
   "metadata": {
    "id": "n-P08UlgqXMY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "26JvFrDZj_ti"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sequence Labeling\n"
   ],
   "metadata": {
    "id": "fqmBnb4CvY5Z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Span representations\n",
    "\n",
    "Span representations are often used as a component in recent models for a number of natural language processing (NLP) tasks, such as question answering.There are three popular schemes as the following!\n",
    "\n",
    "- In the IO scheme, each token in the sequence is labeled as either inside the entity (I) or outside the entity (O).\n",
    "- The BIO scheme, on the other hand, uses two types of tags to indicate whether a token is the beginning (B) of an entity or inside (I) an entity.\n",
    "- BILOU scheme is similar to the BIO scheme, but it includes additional labels to indicate whether a token is the last (L) in an entity, a unit (U) entity, or an outside (O) entity."
   ],
   "metadata": {
    "id": "zdiGDrhp5lMa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Question**:\n",
    "\n",
    "Given the tags: **Loc(ation), Org(aniztion), Cou(rse)**, use IO, BIO and BILOU schemes to label the tags in the sentence:\n",
    "\n",
    "We would set the *Karlsruhe* as Location, the *Artificial Intelligence* as Course and\n",
    "\n",
    "**I live in Karlsruhe and study artificial intelligence at Karlsruhe Institute of Technology**"
   ],
   "metadata": {
    "id": "RklSmja6GxGQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Coding: Named-Entity Recognition\n",
    "\n",
    "In this exercise, we are going to build a transformer-based machine learning model to automatically predict the taggings with a NER dataset.\n",
    "\n"
   ],
   "metadata": {
    "id": "o7jD5TGzve5O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this dataset, entities appear in a chunks of word. The BIO tagging scheme is used to identify boundaries. The BIO tags are further classified into the following classes:\n",
    "\n",
    "*   geo = Geographical Entity\n",
    "*   org = Organization\n",
    "*   per = Person\n",
    "*   gpe = Geopolitical Entity\n",
    "*   tim = Time indicator\n",
    "*   art = Artifact\n",
    "*   eve = Event\n",
    "*   nat = Natural Phenomenon"
   ],
   "metadata": {
    "id": "NTqss6yF53eL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://bwsyncandshare.kit.edu/s/NSXiFZQP3Zk4y8L/download/NER_data.csv"
   ],
   "metadata": {
    "id": "rxemXhP3kPdh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('NER_data.csv', encoding='ISO-8859-1')\n",
    "data['Sentence #'] = data['Sentence #'].fillna(method='ffill')"
   ],
   "metadata": {
    "id": "d7Lz2Ev5ugCv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data.head(10) # Show the first 10 samples. POS stands for part-of-speech tagging task, which is not used here."
   ],
   "metadata": {
    "id": "aMoVM7dJueTU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Unique words and tags\n",
    "vocab_word = list(set(data[\"Word\"].values))\n",
    "vocab_tag = list(set(data[\"Tag\"].values))\n",
    "word2idx = {word: idx + 2 for idx, word in enumerate(vocab_word)}\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<UNK>\"] = 1\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(vocab_tag)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "# Combine sentences\n",
    "def sentence_combine(data):\n",
    "    agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                       s[\"POS\"].values.tolist(),\n",
    "                                                       s[\"Tag\"].values.tolist())]\n",
    "    return data.groupby(\"Sentence #\").apply(agg_func).tolist()\n",
    "\n",
    "sentences = sentence_combine(data)\n",
    "\n",
    "print(sentences[0])"
   ],
   "metadata": {
    "id": "WeBjgZzoupt_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Encode and pad\n",
    "MAX_LEN = 50\n",
    "def encode_sentence(sent):\n",
    "    word_idxs = [word2idx.get(w[0], word2idx[\"<UNK>\"]) for w in sent]\n",
    "    tag_idxs = ... #\n",
    "    word_idxs = word_idxs[:MAX_LEN] + [word2idx[\"<PAD>\"]] * (MAX_LEN - len(word_idxs))\n",
    "    tag_idxs = tag_idxs[:MAX_LEN] + [tag2idx[\"O\"]] * (MAX_LEN - len(tag_idxs))\n",
    "    return torch.tensor(word_idxs), torch.tensor(tag_idxs)\n",
    "\n",
    "encoded = [encode_sentence(s) for s in sentences]\n",
    "X, y = zip(*encoded)\n",
    "\n",
    "# Train/Val/Test Split\n",
    "# Train/Val/Test Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "ylVg1yGWn1XA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ],
   "metadata": {
    "id": "pMv8jt5hvA9_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.stack(X_train), torch.stack(y_train)), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.stack(X_val), torch.stack(y_val)), batch_size=32)\n",
    "test_loader = DataLoader(TensorDataset(torch.stack(X_test), torch.stack(y_test)), batch_size=32)"
   ],
   "metadata": {
    "id": "XnDFA74ku1SF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Transformer Model\n",
    "class NERTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, emb_dim=128, nhead=8, nhid=256, nlayers=2):\n",
    "        super(NERTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=nhid)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "        self.fc = nn.Linear(emb_dim, tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x).permute(1, 0, 2)  # [seq_len, batch, emb_dim]\n",
    "        out = self.transformer(emb).permute(1, 0, 2)  # [batch, seq_len, emb_dim]\n",
    "        return self.fc(out)\n",
    "\n",
    "model = NERTransformer(vocab_size=..., tagset_size=...).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs.view(-1, outputs.shape[-1]), y_batch.view(-1))\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=5)\n"
   ],
   "metadata": {
    "id": "gwZVW54luvj9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluation\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "            total += torch.numel(y_batch)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "val_acc = evaluate(model, val_loader)\n",
    "print(f\"\\nVal Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Plotting loss curves\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "2T16Ak72xWUt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Predict on test set\n",
    "def predict_on_test(model, test_loader, idx2tag, max_batches=3):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (X_batch, y_batch) in enumerate(test_loader):\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predictions = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
    "            X_batch = X_batch.cpu().numpy()\n",
    "            y_batch = y_batch.cpu().numpy()\n",
    "\n",
    "            for x, y_true, y_pred in zip(X_batch, y_batch, predictions):\n",
    "                words = [list(word2idx.keys())[list(word2idx.values()).index(w)] if w in word2idx.values() and w > 1 else \"<UNK>\" for w in x]\n",
    "                tags_true = [idx2tag[idx] for idx in y_true]\n",
    "                tags_pred = [idx2tag[idx] for idx in y_pred]\n",
    "                results.append(list(zip(words, tags_true, tags_pred)))\n",
    "            if i + 1 >= max_batches:\n",
    "                break\n",
    "    return results\n",
    "\n",
    "# Run predictions on test\n",
    "predicted_test_samples = predict_on_test(model, test_loader, idx2tag)\n",
    "\n",
    "# Print examples\n",
    "print(\"\\nSample predictions on test data:\")\n",
    "for i, sentence in enumerate(predicted_test_samples[:5]):\n",
    "    print(f\"\\nSentence {i + 1}:\")\n",
    "    for word, true_tag, pred_tag in sentence:\n",
    "        print(f\"{word:15} | True: {true_tag:10} | Pred: {pred_tag}\")\n"
   ],
   "metadata": {
    "id": "EjlqjBYsoV6W"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
