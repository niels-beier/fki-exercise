{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDJf4y-6u4qZ"
   },
   "source": [
    "# Exercise1 Text Modelling & Natural Language Understanding\n",
    "\n",
    "This exercise sheet will serve as supplementary to lectures and help you to familiarize yourself with Python, Tensroflow, and other commonly used packages.\n",
    "\n",
    "If you are not familiar with Python, you may want to learn more about Python\n",
    "and its basic syntax. Since there are a lot of free and well-written tutorials\n",
    " online, we refer you to one of the following online tutorials:\n",
    "\n",
    "* http://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\n",
    "* https://www.learnpython.org/\n",
    "* https://www.w3schools.com/python/\n",
    "* https://automatetheboringstuff.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJCcM2ZzitcD"
   },
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9EZIi5Ii8qX"
   },
   "source": [
    "A bag of words is a representation of text that describes the occurrence of words within the corpus. Here we are going to have a hands-on exercise to better understand this approach.\n",
    "\n",
    "Given the following sentences:\n",
    "- OpenAI developed ChatGPT.\n",
    "- ChatGPT is a large language model.\n",
    "- Language technology is interestng.\n",
    "\n",
    "**Question**:\n",
    "\n",
    "\n",
    "1.   What is the vocabulary?\n",
    "2.   Encode these sentences with the vocabulary.\n",
    "\n",
    "**Answers**:\n",
    "1.   openai, developed, chatgpt, is, a, large, language, model, technology, interesting\n",
    "2.   - {\"openai\": 1, \"developed\": 1}\n",
    "     - {\"chatgpt\": 1, \"is\": 1, \"a\": 1, \"large\": 1, \"language\": 1, \"model\": 1}\n",
    "     - {\"language\": 1, \"technology\": 1, \"is\": 1, \"interesting\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmCYuwngiydr"
   },
   "source": [
    "### N-gram Language Model\n",
    "\n",
    "Next, we are going to have a coding exercise about the n-gram language model. We have a corpus that is one chapter from a book and has almost 1000 lines. In this exercise, rather than building every function from scratch, we are going to use the toolkit NLTK, which is a suite of open-source Python modules, data sets, and tutorials supporting research and development in Natural Language Processing.\n",
    "\n",
    "For more information about NLTK, please refer to the official documentation: https://www.nltk.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:31:17.756066Z",
     "start_time": "2025-05-05T14:31:17.375759Z"
    },
    "id": "yzweuCiE9478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-05 16:31:17--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8000::154, 2606:50c0:8002::154, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 278779 (272K) [text/plain]\r\n",
      "Saving to: ‘botchan.txt’\r\n",
      "\r\n",
      "botchan.txt         100%[===================>] 272.25K  --.-KB/s    in 0.04s   \r\n",
      "\r\n",
      "2025-05-05 16:31:17 (7.53 MB/s) - ‘botchan.txt’ saved [278779/278779]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Firstly, let's download the data for this exercises\n",
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:36:02.961013Z",
     "start_time": "2025-05-05T14:36:02.408035Z"
    },
    "id": "t8PT3d4E-OH2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/niels/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you are running with colab, this toolkit should be installed.\n",
    "import nltk\n",
    "print(nltk.__version__)\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:37:27.218311Z",
     "start_time": "2025-05-05T14:37:27.215247Z"
    },
    "id": "DCfC-cLa40wP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~“”-’‘—\n",
      "!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~“”-’‘—\n"
     ]
    }
   ],
   "source": [
    "# Load punctuations that to be removed in later preprocessing.\n",
    "import nltk, re, pprint, string\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "puncs = string.punctuation +'“'+'”'+'-'+'’'+'‘'+'—'\n",
    "print(puncs)\n",
    "\n",
    "puncs = puncs.replace('.', '')\n",
    "print(puncs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:43:04.565376Z",
     "start_time": "2025-05-05T14:43:02.996353Z"
    },
    "id": "y6lL2sNjFlUF"
   },
   "outputs": [],
   "source": [
    "# Load the corpus\n",
    "file = open('botchan.txt', encoding = 'utf8').read()\n",
    "#preprocess data\n",
    "file_nl_removed = \"\"\n",
    "for line in file:\n",
    "  line_nl_removed = line.replace(\"\\n\", \" \")           #removes newlines\n",
    "  file_nl_removed += line_nl_removed\n",
    "\n",
    "file_p = \"\".join([char for char in file_nl_removed if char not in puncs])   #removes all special characters. Save the data as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:43:07.158874Z",
     "start_time": "2025-05-05T14:43:07.153597Z"
    },
    "id": "WmWKBl7n6Sql"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffProject Gutenbergs Botchan Master Darling by Kinnosuke Natsume This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever.  You may copy it give it away or reuse it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org Title Botchan Master Darling Author Kinnosuke Natsume Translator Yasotaro Morri Posting Date October 14 2012 EBook 8868 Release Date September 2005 First Posted August 17 2003 Language English  START OF THIS PROJECT GUTENBERG EBOOK BOTCHAN MASTER DARLING  Produced by David Starner and the Online Distributed Proofreading Team BOTCHAN MASTER DARLING By The Late Mr. Kinnosuke Natsume TRANSLATED By Yasotaro Morri Revised by J. R. KENNEDY 1919 A NOTE BY THE TRANSLATOR No translation can expect to equal much less to excel the original. The excellence of a translation can only be judged by noting how far it has succeeded in reproducing the original tone colors style the delicacy of senti'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_p[:1000] # Display the first 1000 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:46:56.907206Z",
     "start_time": "2025-05-05T14:46:56.512380Z"
    },
    "id": "VV3pSjmY6Lgh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 2787\n",
      "The number of tokens is 53568\n",
      "The average number of tokens per sentence is 19\n",
      "The number of unique tokens are 6293\n"
     ]
    }
   ],
   "source": [
    "# Check some statistics about the data. Here you need to complete the codes with reference: https://www.nltk.org/api/nltk.tokenize.html\n",
    "sents = sent_tokenize(file_p)\n",
    "print(\"The number of sentences is\", len(sents)) #prints the number of sentences\n",
    "\n",
    "words = word_tokenize(file_p)\n",
    "print(\"The number of tokens is\", len(words)) #prints the number of tokens\n",
    "\n",
    "average_tokens = round(len(words)/len(sents))\n",
    "print(\"The average number of tokens per sentence is\", average_tokens) #prints the average number of tokens per sentence\n",
    "\n",
    "unique_tokens = set(words)\n",
    "print(\"The number of unique tokens are\", len(unique_tokens)) #prints the number of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:48:56.696625Z",
     "start_time": "2025-05-05T14:48:56.362338Z"
    },
    "id": "sfEL_tJTCfWn"
   },
   "outputs": [],
   "source": [
    "# Now let's collect ngrams\n",
    "from nltk.util import ngrams #https://www.nltk.org/_modules/nltk/util.html#ngrams\n",
    "unigram=[]\n",
    "bigram=[]\n",
    "trigram=[]\n",
    "fourgram=[]\n",
    "tokenized_text = []\n",
    "\n",
    "for sentence in sents:\n",
    "    sentence = sentence.lower() # Lower the sentence for simplification\n",
    "    sequence = word_tokenize(sentence) # Word-level\n",
    "    for word in sequence:\n",
    "        if (word =='.'):\n",
    "            sequence.remove(word)\n",
    "        else:\n",
    "            unigram.append(word)\n",
    "    tokenized_text.append(sequence)\n",
    "    bigram.extend(list(ngrams(sequence, 2)))\n",
    "    trigram.extend(list(ngrams(sequence, 3)))\n",
    "    fourgram.extend(list(ngrams(sequence, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:48:59.420134Z",
     "start_time": "2025-05-05T14:48:59.415914Z"
    },
    "id": "tZVHFOC2Ci2k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unigram, bigram, trigram and fourgram are 50796, 48009, 45246, 42515\n"
     ]
    }
   ],
   "source": [
    "print(f'Lengths of unigram, bigram, trigram and fourgram are {len(unigram)}, {len(bigram)}, {len(trigram)}, {len(fourgram)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:49:08.345953Z",
     "start_time": "2025-05-05T14:49:08.339092Z"
    },
    "id": "u6lAxDhKZcPa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\\ufeffproject', 'gutenbergs', 'botchan', 'master', 'darling', 'by', 'kinnosuke', 'natsume', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever'], ['you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www.gutenberg.org', 'title', 'botchan', 'master', 'darling', 'author', 'kinnosuke', 'natsume', 'translator', 'yasotaro', 'morri', 'posting', 'date', 'october', '14', '2012', 'ebook', '8868', 'release', 'date', 'september', '2005', 'first', 'posted', 'august', '17', '2003', 'language', 'english', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'botchan', 'master', 'darling', 'produced', 'by', 'david', 'starner', 'and', 'the', 'online', 'distributed', 'proofreading', 'team', 'botchan', 'master', 'darling', 'by', 'the', 'late', 'mr.', 'kinnosuke', 'natsume', 'translated', 'by', 'yasotaro', 'morri', 'revised', 'by', 'j.', 'r.', 'kennedy', '1919', 'a', 'note', 'by', 'the', 'translator', 'no', 'translation', 'can', 'expect', 'to', 'equal', 'much', 'less', 'to', 'excel', 'the', 'original']]\n",
      "['\\ufeffproject', 'gutenbergs', 'botchan', 'master', 'darling']\n",
      "[('\\ufeffproject', 'gutenbergs'), ('gutenbergs', 'botchan'), ('botchan', 'master'), ('master', 'darling'), ('darling', 'by')]\n",
      "[('\\ufeffproject', 'gutenbergs', 'botchan'), ('gutenbergs', 'botchan', 'master'), ('botchan', 'master', 'darling'), ('master', 'darling', 'by'), ('darling', 'by', 'kinnosuke')]\n",
      "[('\\ufeffproject', 'gutenbergs', 'botchan', 'master'), ('gutenbergs', 'botchan', 'master', 'darling'), ('botchan', 'master', 'darling', 'by'), ('master', 'darling', 'by', 'kinnosuke'), ('darling', 'by', 'kinnosuke', 'natsume')]\n"
     ]
    }
   ],
   "source": [
    "# Inspect\n",
    "print(tokenized_text[:2])\n",
    "\n",
    "print(unigram[:5])\n",
    "print(bigram[:5])\n",
    "print(trigram[:5])\n",
    "print(fourgram[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:49:19.908923Z",
     "start_time": "2025-05-05T14:49:19.699919Z"
    },
    "id": "aSbUY7cz-BhC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common unigram:  [('the', 2755), ('i', 1714), ('to', 1472), ('and', 1310), ('of', 1258)]\n",
      "Most common bigrams:  [(('of', 'the'), 316), (('in', 'the'), 223), (('to', 'the'), 189), (('red', 'shirt'), 174), (('i', 'was'), 137)]\n",
      "Most common trigrams:  [(('i', 'did', 'not'), 39), (('it', 'would', 'be'), 32), (('i', 'could', 'not'), 28), (('of', 'red', 'shirt'), 24), (('to', 'the', 'school'), 21)]\n",
      "Most common fourgrams:  [(('one', 'sen', 'and', 'a'), 13), (('sen', 'and', 'a', 'half'), 13), (('the', 'project', 'gutenberg', 'literary'), 13), (('project', 'gutenberg', 'literary', 'archive'), 13), (('gutenberg', 'literary', 'archive', 'foundation'), 13)]\n"
     ]
    }
   ],
   "source": [
    "freq_uni = nltk.FreqDist(unigram)\n",
    "freq_bi = nltk.FreqDist(bigram)\n",
    "freq_tri = nltk.FreqDist(trigram)\n",
    "freq_four = nltk.FreqDist(fourgram)\n",
    "\n",
    "print (\"Most common unigram: \", freq_uni.most_common(5))\n",
    "print (\"Most common bigrams: \", freq_bi.most_common(5))\n",
    "print (\"Most common trigrams: \", freq_tri.most_common(5))\n",
    "print (\"Most common fourgrams: \", freq_four.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T14:52:52.494023Z",
     "start_time": "2025-05-05T14:52:52.469941Z"
    },
    "id": "GuyWRWLqcvwD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching  ('like', 'the')\n",
      "Found  ['cheap', 'undue', 'dwarf', 'name', 'wrestling', 'inmates', 'tradespeople', 'wrestling']\n",
      "Most Common words with counts is:  [('wrestling', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Predict the next word\n",
    "str1 = 'i like the'\n",
    "token1 = word_tokenize(str1)\n",
    "\n",
    "num_gram = 2  # what if 4, 5\n",
    "\n",
    "results = list(ngrams(token1, num_gram))\n",
    "search=results[-1]\n",
    "print('Searching ', search)\n",
    "preds=[]\n",
    "\n",
    "# Naive search with a for loop\n",
    "\n",
    "for each in trigram:\n",
    "  if each[:-1]==search:\n",
    "    preds.append(each[-1])\n",
    "print('Found ', preds)\n",
    "\n",
    "# What to predict?\n",
    "from collections import Counter\n",
    "word_counts = Counter(preds)\n",
    "print('Most Common words with counts is: ', word_counts.most_common(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73PSq7f0g_Ft"
   },
   "outputs": [],
   "source": [
    "# Other popular ngram LM with scoring functions\n",
    "\n",
    "# https://github.com/kmario23/KenLM-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8HPYpBti4Fh"
   },
   "source": [
    "### Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-LxM6kpGqA8"
   },
   "source": [
    "Byte Pair Encoding (BPE) was initially developed as an algorithm to compress data, and now is commonly used in natural language processing tasks such as machine translation and text generation. BPE starts by computing the unique set of characters used in the corpus, then building the vocabulary by taking all the symbols used to write those words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEtih0VP9tws"
   },
   "source": [
    "#### Calcuation\n",
    "\n",
    "Here we are going to implement BPE with a simple corpus step by step.\n",
    "\n",
    "Given the corpus like the following:\n",
    "\n",
    "- happy * 3\n",
    "- lucky * 2\n",
    "- cool * 2\n",
    "- good * 2\n",
    "\n",
    "You can inteprete each word as a single sentence. For example, the split of the words happy as {h_a_p_p_y_/w}\n",
    "\n",
    "**Questions**:\n",
    "\n",
    "\n",
    "1.   What is the initial vocabulary? Hint: don't forget the token **/w** at the end of each word.\n",
    "2.   What is the merge operation in the first iteration?\n",
    "3.   Let's say we would like to have a vocabulary with size 15. Calculate the vocabulary and the merging rules.\n",
    "\n",
    "**Answers**:\n",
    "1. {h, a, p, y, l, u, c, k, o, g, /w}\n",
    "2. y_/w -> y/w\n",
    "3. <img src=\"IMG_1049.jpg\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGab9Cg19nzm"
   },
   "source": [
    "\n",
    "#### Coding\n",
    "Next we are going to build a BPE model with [SentencePiece](https://github.com/google/sentencepiece/tree/master)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:00:15.824950Z",
     "start_time": "2025-05-06T10:00:15.175936Z"
    },
    "id": "eUMvy7QKjYYg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/niels/coding/fki/.venv/lib/python3.12/site-packages (0.2.0)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Coding exercise\n",
    "!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:00:24.339713Z",
     "start_time": "2025-05-06T10:00:24.227555Z"
    },
    "id": "bkmTtaUqpoxi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Project Gutenberg's Botchan (Master Darling), by Kin-nosuke Natsume\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 botchan.txt # Display the first three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:01:12.350505Z",
     "start_time": "2025-05-06T10:01:12.095667Z"
    },
    "id": "otyovEjXqGLA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁t', 'est']\n",
      "[208, 31, 9, 434, 601]\n",
      "This is a test\n",
      "This is a test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71845 num_tokens=20449 num_tokens/piece=5.21259\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3921 obj=8.66278 num_tokens=20450 num_tokens/piece=5.21551\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2940 obj=8.96602 num_tokens=22797 num_tokens/piece=7.75408\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2940 obj=8.88624 num_tokens=22798 num_tokens/piece=7.75442\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2205 obj=9.2744 num_tokens=25530 num_tokens/piece=11.5782\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2205 obj=9.18615 num_tokens=25533 num_tokens/piece=11.5796\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2200 obj=9.18811 num_tokens=25552 num_tokens/piece=11.6145\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2200 obj=9.18747 num_tokens=25552 num_tokens/piece=11.6145\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix='m', vocab_size=2000)\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(sp.encode_as_pieces('This is a test')))\n",
    "print(sp.decode_ids(sp.encode_as_ids('This is a test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:01:53.022293Z",
     "start_time": "2025-05-06T10:01:53.018948Z"
    },
    "id": "ZXwomFHwqUjN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "<unk>\n",
      "<s>\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "# returns vocab size\n",
    "print(sp.vocab_size())\n",
    "\n",
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for id in range(3):\n",
    "  print(sp.id_to_piece(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:03:12.348633Z",
     "start_time": "2025-05-06T10:03:11.301925Z"
    },
    "id": "n-P08UlgqXMY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'T', 'h', 'i', 's', '▁', 'i', 's', '▁', 'e', 'x', 'er', 'c', 'i', 's', 'e', '▁', '1']\n",
      "['▁This', '▁is', '▁ex', 'er', 'c', 'is', 'e', '▁', '1']\n",
      "['▁This', '▁is', '▁exercise', '▁1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 100\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71845 num_tokens=20449 num_tokens/piece=5.21259\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3921 obj=8.66278 num_tokens=20450 num_tokens/piece=5.21551\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2940 obj=8.96602 num_tokens=22797 num_tokens/piece=7.75408\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2940 obj=8.88624 num_tokens=22798 num_tokens/piece=7.75442\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2205 obj=9.2744 num_tokens=25530 num_tokens/piece=11.5782\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2205 obj=9.18615 num_tokens=25533 num_tokens/piece=11.5796\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1653 obj=9.6456 num_tokens=28789 num_tokens/piece=17.4162\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1653 obj=9.55867 num_tokens=28792 num_tokens/piece=17.418\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1239 obj=10.1208 num_tokens=32288 num_tokens/piece=26.0597\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1239 obj=10.0257 num_tokens=32288 num_tokens/piece=26.0597\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=929 obj=10.6049 num_tokens=35498 num_tokens/piece=38.211\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=929 obj=10.5054 num_tokens=35498 num_tokens/piece=38.211\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=696 obj=11.0822 num_tokens=38632 num_tokens/piece=55.5057\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=696 obj=10.9787 num_tokens=38634 num_tokens/piece=55.5086\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=522 obj=11.5785 num_tokens=41419 num_tokens/piece=79.3467\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=522 obj=11.4695 num_tokens=41419 num_tokens/piece=79.3467\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=391 obj=12.0921 num_tokens=44326 num_tokens/piece=113.366\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=391 obj=11.9655 num_tokens=44326 num_tokens/piece=113.366\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=293 obj=12.6316 num_tokens=47512 num_tokens/piece=162.157\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=293 obj=12.473 num_tokens=47512 num_tokens/piece=162.157\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=219 obj=13.2109 num_tokens=50291 num_tokens/piece=229.639\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=219 obj=13.0353 num_tokens=50291 num_tokens/piece=229.639\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=164 obj=13.9051 num_tokens=54275 num_tokens/piece=330.945\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=164 obj=13.6617 num_tokens=54275 num_tokens/piece=330.945\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=123 obj=14.7382 num_tokens=59928 num_tokens/piece=487.22\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=123 obj=14.3544 num_tokens=59928 num_tokens/piece=487.22\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=110 obj=14.7723 num_tokens=61745 num_tokens/piece=561.318\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=110 obj=14.6553 num_tokens=61745 num_tokens/piece=561.318\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3923 obj=8.71845 num_tokens=20449 num_tokens/piece=5.21259\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3921 obj=8.66278 num_tokens=20450 num_tokens/piece=5.21551\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2940 obj=8.96602 num_tokens=22797 num_tokens/piece=7.75408\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2940 obj=8.88624 num_tokens=22798 num_tokens/piece=7.75442\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2205 obj=9.2744 num_tokens=25530 num_tokens/piece=11.5782\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2205 obj=9.18615 num_tokens=25533 num_tokens/piece=11.5796\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1653 obj=9.6456 num_tokens=28789 num_tokens/piece=17.4162\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1653 obj=9.55867 num_tokens=28792 num_tokens/piece=17.418\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1239 obj=10.1208 num_tokens=32288 num_tokens/piece=26.0597\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=1239 obj=10.0257 num_tokens=32288 num_tokens/piece=26.0597\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=929 obj=10.6049 num_tokens=35498 num_tokens/piece=38.211\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=929 obj=10.5054 num_tokens=35498 num_tokens/piece=38.211\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=696 obj=11.0822 num_tokens=38632 num_tokens/piece=55.5057\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=696 obj=10.9787 num_tokens=38634 num_tokens/piece=55.5086\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=550 obj=11.4617 num_tokens=40909 num_tokens/piece=74.38\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=550 obj=11.3802 num_tokens=40909 num_tokens/piece=74.38\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: botchan.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: botchan.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 4288 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=274252\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.957% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99957\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4288 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=144687\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 16112 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4288\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 9165\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 9165 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5926 obj=10.5283 num_tokens=18931 num_tokens/piece=3.19457\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5232 obj=8.64492 num_tokens=19009 num_tokens/piece=3.63322\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "# The impact of vocab size.\n",
    "for vocab_size in [100, 500, 5000]:\n",
    "  spm.SentencePieceTrainer.train(input='botchan.txt', model_prefix='m', vocab_size=vocab_size)\n",
    "  sp = spm.SentencePieceProcessor()\n",
    "  sp.load('m.model')\n",
    "  print(sp.encode_as_pieces('This is exercise 1'))\n",
    "\n",
    " # What if vocab_size is 50?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26JvFrDZj_ti"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqmBnb4CvY5Z"
   },
   "source": [
    "## Sequence Labeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdiGDrhp5lMa"
   },
   "source": [
    "### Span representations\n",
    "\n",
    "Span representations are often used as a component in recent models for a number of natural language processing (NLP) tasks, such as question answering.There are three popular schemes as the following!\n",
    "\n",
    "- In the IO scheme, each token in the sequence is labeled as either inside the entity (I) or outside the entity (O).\n",
    "- The BIO scheme, on the other hand, uses two types of tags to indicate whether a token is the beginning (B) of an entity or inside (I) an entity.\n",
    "- BILOU scheme is similar to the BIO scheme, but it includes additional labels to indicate whether a token is the last (L) in an entity, a unit (U) entity, or an outside (O) entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RklSmja6GxGQ"
   },
   "source": [
    "**Question**:\n",
    "\n",
    "Given the tags: **Loc(ation), Org(aniztion), Cou(rse)**, use IO, BIO and BILOU schemes to label the tags in the sentence:\n",
    "\n",
    "We would set the *Karlsruhe* as Location, the *Artificial Intelligence* as Course and *Institute of Technology* as Organization\n",
    "\n",
    "**I live in Karlsruhe and study artificial intelligence at Karlsruhe Institute of Technology**\n",
    "\n",
    "**Answer**:\n",
    "1. \"I\": O, \"live\": O, \"in\": O, \"Karlsruhe\": Loc, \"and\": O, \"study\": O, \"artificial\": Cou, \"intelligence\": Cou, \"at\": O, \"Karlsruhe\": Org, \"Institute\": Org, \"of\": Org, \"Technology\": Org\n",
    "2. \"I\": O, \"live\": O, \"in\": O, \"Karlsruhe\": B-Loc, \"and\": O, \"study\": O, \"artificial\": B-Cou, \"intelligence\": I-Cou, \"at\": O, \"Karlsruhe\": B-Loc, \"Institute\": B-Org, \"of\": I-Org, \"Technology\": L-Org\n",
    "3. \"I\": O, \"live\": O, \"in\": O, \"Karlsruhe\": U-Loc, \"and\": O, \"study\": O, \"artificial\": B-Cou, \"intelligence\": L-Cou, \"at\": O, \"Karlsruhe\": U-Loc, \"Institute\": B-Org, \"of\": I-Org, \"Technology\": L-Org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7jD5TGzve5O"
   },
   "source": [
    "### Coding: Named-Entity Recognition\n",
    "\n",
    "In this exercise, we are going to build a transformer-based machine learning model to automatically predict the taggings with a NER dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTqss6yF53eL"
   },
   "source": [
    "In this dataset, entities appear in a chunks of word. The BIO tagging scheme is used to identify boundaries. The BIO tags are further classified into the following classes:\n",
    "\n",
    "*   geo = Geographical Entity\n",
    "*   org = Organization\n",
    "*   per = Person\n",
    "*   gpe = Geopolitical Entity\n",
    "*   tim = Time indicator\n",
    "*   art = Artifact\n",
    "*   eve = Event\n",
    "*   nat = Natural Phenomenon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:14:30.846632Z",
     "start_time": "2025-05-06T10:14:29.894870Z"
    },
    "id": "rxemXhP3kPdh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-06 12:14:29--  https://bwsyncandshare.kit.edu/s/NSXiFZQP3Zk4y8L/download/NER_data.csv\r\n",
      "Resolving bwsyncandshare.kit.edu (bwsyncandshare.kit.edu)... 2a00:1398:b::8d03:8781, 141.3.135.129\r\n",
      "Connecting to bwsyncandshare.kit.edu (bwsyncandshare.kit.edu)|2a00:1398:b::8d03:8781|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 14159575 (14M) [text/csv]\r\n",
      "Saving to: ‘NER_data.csv’\r\n",
      "\r\n",
      "NER_data.csv        100%[===================>]  13.50M  65.7MB/s    in 0.2s    \r\n",
      "\r\n",
      "2025-05-06 12:14:30 (65.7 MB/s) - ‘NER_data.csv’ saved [14159575/14159575]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://bwsyncandshare.kit.edu/s/NSXiFZQP3Zk4y8L/download/NER_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:39:37.805655Z",
     "start_time": "2025-05-06T10:39:35.251190Z"
    },
    "id": "d7Lz2Ev5ugCv"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('NER_data.csv', encoding='ISO-8859-1')\n",
    "data['Sentence #'] = data['Sentence #'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:39:40.680004Z",
     "start_time": "2025-05-06T10:39:40.671164Z"
    },
    "id": "aMoVM7dJueTU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1  Sentence: 1             of   IN      O\n",
       "2  Sentence: 1  demonstrators  NNS      O\n",
       "3  Sentence: 1           have  VBP      O\n",
       "4  Sentence: 1        marched  VBN      O\n",
       "5  Sentence: 1        through   IN      O\n",
       "6  Sentence: 1         London  NNP  B-geo\n",
       "7  Sentence: 1             to   TO      O\n",
       "8  Sentence: 1        protest   VB      O\n",
       "9  Sentence: 1            the   DT      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10) # Show the first 10 samples. POS stands for part-of-speech tagging task, which is not used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:39:45.832433Z",
     "start_time": "2025-05-06T10:39:42.568692Z"
    },
    "id": "WeBjgZzoupt_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63949/1092368143.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return data.groupby(\"Sentence #\").apply(agg_func).tolist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Unique words and tags\n",
    "vocab_word = list(set(data[\"Word\"].values))\n",
    "vocab_tag = list(set(data[\"Tag\"].values))\n",
    "word2idx = {word: idx + 2 for idx, word in enumerate(vocab_word)}\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<UNK>\"] = 1\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(vocab_tag)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "# Combine sentences\n",
    "def sentence_combine(data):\n",
    "    agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                       s[\"POS\"].values.tolist(),\n",
    "                                                       s[\"Tag\"].values.tolist())]\n",
    "    return data.groupby(\"Sentence #\").apply(agg_func).tolist()\n",
    "\n",
    "sentences = sentence_combine(data)\n",
    "\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:39:49.358166Z",
     "start_time": "2025-05-06T10:39:47.532643Z"
    },
    "id": "ylVg1yGWn1XA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Encode and pad\n",
    "MAX_LEN = 50\n",
    "def encode_sentence(sent):\n",
    "    word_idxs = [word2idx.get(w[0], word2idx[\"<UNK>\"]) for w in sent]\n",
    "    tag_idxs =  [tag2idx.get(w[1], tag2idx[\"O\"]) for w in sent]#\n",
    "    word_idxs = word_idxs[:MAX_LEN] + [word2idx[\"<PAD>\"]] * (MAX_LEN - len(word_idxs))\n",
    "    tag_idxs = tag_idxs[:MAX_LEN] + [tag2idx[\"O\"]] * (MAX_LEN - len(tag_idxs))\n",
    "    return torch.tensor(word_idxs), torch.tensor(tag_idxs)\n",
    "\n",
    "encoded = [encode_sentence(s) for s in sentences]\n",
    "X, y = zip(*encoded)\n",
    "\n",
    "# Train/Val/Test Split\n",
    "# Train/Val/Test Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=1/3, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:39:51.181467Z",
     "start_time": "2025-05-06T10:39:51.177370Z"
    },
    "id": "pMv8jt5hvA9_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33571\n",
      "33571\n",
      "tensor([28659, 27663, 19968, 15509, 18624, 13760, 12205, 16932, 14050, 22085,\n",
      "        27663,  8361, 15191,  1530, 27344, 22964,  9930,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T10:59:08.255860Z",
     "start_time": "2025-05-06T10:59:08.174246Z"
    },
    "id": "XnDFA74ku1SF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.stack(X_train), torch.stack(y_train)), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(torch.stack(X_val), torch.stack(y_val)), batch_size=32)\n",
    "test_loader = DataLoader(TensorDataset(torch.stack(X_test), torch.stack(y_test)), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T11:05:16.353118Z",
     "start_time": "2025-05-06T10:59:10.521034Z"
    },
    "id": "gwZVW54luvj9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niels/coding/fki/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.0064, Val Loss: 0.0000\n",
      "Epoch 2/5, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 3/5, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 4/5, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 5/5, Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Transformer Model\n",
    "class NERTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, emb_dim=128, nhead=8, nhid=256, nlayers=2):\n",
    "        super(NERTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dim_feedforward=nhid)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "        self.fc = nn.Linear(emb_dim, tagset_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x).permute(1, 0, 2)  # [seq_len, batch, emb_dim]\n",
    "        out = self.transformer(emb).permute(1, 0, 2)  # [batch, seq_len, emb_dim]\n",
    "        return self.fc(out)\n",
    "\n",
    "model = NERTransformer(vocab_size=len(word2idx), tagset_size=len(tag2idx)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs.view(-1, outputs.shape[-1]), y_batch.view(-1))\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T11:05:32.212308Z",
     "start_time": "2025-05-06T11:05:29.771197Z"
    },
    "id": "2T16Ak72xWUt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 1.0000\n",
      "\n",
      "Test Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "            total += torch.numel(y_batch)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "val_acc = evaluate(model, val_loader)\n",
    "print(f\"\\nVal Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Plotting loss curves\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T11:05:52.004914Z",
     "start_time": "2025-05-06T11:05:43.944948Z"
    },
    "id": "EjlqjBYsoV6W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions on test data:\n",
      "\n",
      "Sentence 1:\n",
      "From            | True: O          | Pred: O\n",
      "2004            | True: O          | Pred: O\n",
      "to              | True: O          | Pred: O\n",
      "2007            | True: O          | Pred: O\n",
      ",               | True: O          | Pred: O\n",
      "the             | True: O          | Pred: O\n",
      "economy         | True: O          | Pred: O\n",
      "grew            | True: O          | Pred: O\n",
      "about           | True: O          | Pred: O\n",
      "10              | True: O          | Pred: O\n",
      "%               | True: O          | Pred: O\n",
      "per             | True: O          | Pred: O\n",
      "year            | True: O          | Pred: O\n",
      ",               | True: O          | Pred: O\n",
      "driven          | True: O          | Pred: O\n",
      "largely         | True: O          | Pred: O\n",
      "by              | True: O          | Pred: O\n",
      "an              | True: O          | Pred: O\n",
      "expansion       | True: O          | Pred: O\n",
      "in              | True: O          | Pred: O\n",
      "the             | True: O          | Pred: O\n",
      "garment         | True: O          | Pred: O\n",
      "sector          | True: O          | Pred: O\n",
      ",               | True: O          | Pred: O\n",
      "construction    | True: O          | Pred: O\n",
      ",               | True: O          | Pred: O\n",
      "agriculture     | True: O          | Pred: O\n",
      ",               | True: O          | Pred: O\n",
      "and             | True: O          | Pred: O\n",
      "tourism         | True: O          | Pred: O\n",
      ".               | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "\n",
      "Sentence 2:\n",
      "Earlier         | True: O          | Pred: O\n",
      "this            | True: O          | Pred: O\n",
      "week            | True: O          | Pred: O\n",
      ",               | True: O          | Pred: O\n",
      "the             | True: O          | Pred: O\n",
      "African         | True: O          | Pred: O\n",
      "Union           | True: O          | Pred: O\n",
      "dispatched      | True: O          | Pred: O\n",
      "former          | True: O          | Pred: O\n",
      "South           | True: O          | Pred: O\n",
      "African         | True: O          | Pred: O\n",
      "president       | True: O          | Pred: O\n",
      "Thabo           | True: O          | Pred: O\n",
      "Mbeki           | True: O          | Pred: O\n",
      "to              | True: O          | Pred: O\n",
      "seek            | True: O          | Pred: O\n",
      "a               | True: O          | Pred: O\n",
      "solution        | True: O          | Pred: O\n",
      ",               | True: O          | Pred: O\n",
      "but             | True: O          | Pred: O\n",
      "after           | True: O          | Pred: O\n",
      "two             | True: O          | Pred: O\n",
      "days            | True: O          | Pred: O\n",
      "of              | True: O          | Pred: O\n",
      "talks           | True: O          | Pred: O\n",
      ",               | True: O          | Pred: O\n",
      "the             | True: O          | Pred: O\n",
      "situation       | True: O          | Pred: O\n",
      "was             | True: O          | Pred: O\n",
      "not             | True: O          | Pred: O\n",
      "resolved        | True: O          | Pred: O\n",
      ".               | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "\n",
      "Sentence 3:\n",
      "China           | True: O          | Pred: O\n",
      "'s              | True: O          | Pred: O\n",
      "state           | True: O          | Pred: O\n",
      "news            | True: O          | Pred: O\n",
      "agency          | True: O          | Pred: O\n",
      "says            | True: O          | Pred: O\n",
      "scientists      | True: O          | Pred: O\n",
      "have            | True: O          | Pred: O\n",
      "developed       | True: O          | Pred: O\n",
      "two             | True: O          | Pred: O\n",
      "vaccines        | True: O          | Pred: O\n",
      "to              | True: O          | Pred: O\n",
      "prevent         | True: O          | Pred: O\n",
      "the             | True: O          | Pred: O\n",
      "spread          | True: O          | Pred: O\n",
      "of              | True: O          | Pred: O\n",
      "the             | True: O          | Pred: O\n",
      "deadly          | True: O          | Pred: O\n",
      "H5N1            | True: O          | Pred: O\n",
      "strain          | True: O          | Pred: O\n",
      "of              | True: O          | Pred: O\n",
      "bird            | True: O          | Pred: O\n",
      "flu             | True: O          | Pred: O\n",
      ".               | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "\n",
      "Sentence 4:\n",
      "He              | True: O          | Pred: O\n",
      "said            | True: O          | Pred: O\n",
      "Americans       | True: O          | Pred: O\n",
      "are             | True: O          | Pred: O\n",
      "thankful        | True: O          | Pred: O\n",
      "for             | True: O          | Pred: O\n",
      "their           | True: O          | Pred: O\n",
      "sacrifices      | True: O          | Pred: O\n",
      ".               | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "\n",
      "Sentence 5:\n",
      "The             | True: O          | Pred: O\n",
      "letter          | True: O          | Pred: O\n",
      "fueled          | True: O          | Pred: O\n",
      "charges         | True: O          | Pred: O\n",
      "of              | True: O          | Pred: O\n",
      "racism          | True: O          | Pred: O\n",
      "and             | True: O          | Pred: O\n",
      "was             | True: O          | Pred: O\n",
      "condemned       | True: O          | Pred: O\n",
      "by              | True: O          | Pred: O\n",
      "some            | True: O          | Pred: O\n",
      "lawmakers       | True: O          | Pred: O\n",
      "and             | True: O          | Pred: O\n",
      "human           | True: O          | Pred: O\n",
      "rights          | True: O          | Pred: O\n",
      "activists       | True: O          | Pred: O\n",
      ".               | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n",
      "<UNK>           | True: O          | Pred: O\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "def predict_on_test(model, test_loader, idx2tag, max_batches=3):\n",
    "    model.eval()\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (X_batch, y_batch) in enumerate(test_loader):\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            predictions = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
    "            X_batch = X_batch.cpu().numpy()\n",
    "            y_batch = y_batch.cpu().numpy()\n",
    "\n",
    "            for x, y_true, y_pred in zip(X_batch, y_batch, predictions):\n",
    "                words = [list(word2idx.keys())[list(word2idx.values()).index(w)] if w in word2idx.values() and w > 1 else \"<UNK>\" for w in x]\n",
    "                tags_true = [idx2tag[idx] for idx in y_true]\n",
    "                tags_pred = [idx2tag[idx] for idx in y_pred]\n",
    "                results.append(list(zip(words, tags_true, tags_pred)))\n",
    "            if i + 1 >= max_batches:\n",
    "                break\n",
    "    return results\n",
    "\n",
    "# Run predictions on test\n",
    "predicted_test_samples = predict_on_test(model, test_loader, idx2tag)\n",
    "\n",
    "# Print examples\n",
    "print(\"\\nSample predictions on test data:\")\n",
    "for i, sentence in enumerate(predicted_test_samples[:5]):\n",
    "    print(f\"\\nSentence {i + 1}:\")\n",
    "    for word, true_tag, pred_tag in sentence:\n",
    "        print(f\"{word:15} | True: {true_tag:10} | Pred: {pred_tag}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
